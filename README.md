# PySpark Code Explainer

A lightweight, containerized AI tool that explains PySpark code using a FastAPI backend, Streamlit frontend, and Gemini 1.5 Flash/Pro.

---

## ğŸš€ Overview
This project provides a simple web UI where users can paste PySpark code and receive a clear explanation generated by an LLM.

**Architecture:**
- **FastAPI backend** â†’ exposes `/explain/pyspark`
- **Gemini API** â†’ generates explanations
- **Streamlit frontend** â†’ user interface
- **Docker Compose** â†’ orchestration

---

## ğŸ§± Project Structure
```text
.
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ main.py           # orchestrates FastAPI + routes + startup events
â”‚   â”‚   â”œâ”€â”€ llm.py            # GeminiClient
â”‚   â”‚   â”œâ”€â”€ tasks.py          # Celery tasks for async LLM calls
â”‚   â”‚   â”œâ”€â”€ routes.py         # /explain/pyspark + /status/<job_id>
â”‚   â”‚   â”œâ”€â”€ schemas.py        # request & response schemas
â”‚   â”‚   â”œâ”€â”€ config.py         # Settings (Pydantic)
â”‚   â”‚   â””â”€â”€ cache.py          # Redis caching helpers
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â””â”€â”€ README.md
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ app.py
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â””â”€â”€ README.md
â”œâ”€â”€ docker-compose.yml
â””â”€â”€ README.md
```

---

## ğŸƒ Running the Project

### 1. Create the backend `.env`
```bash
GEMINI_API_KEY=your_key_here
```

### 2. Start the app
```bash
docker compose up --build
```

### 3. Open the UI  
http://localhost:8501

---

## ğŸ“¡ API Endpoint

**POST** `/explain/pyspark`

**Request Body**
```json
{
  "code": "df = spark.read.csv('data.csv')"
}
```

**Response**
```json
{
  "explanation": "This code reads a CSV file named 'data.csv' into a Spark DataFrame..."
}
```

---

## ğŸ›  Tech Used
- FastAPI  
- Streamlit  
- Gemini API (google-generativeai)  
- Docker / Docker Compose  

---

# ğŸš€ Project Roadmap (4 Stages)

This project is designed to grow into a full **AI-powered ETL intelligence system**.

---

## ğŸŸ¦ Stage 1 â€” Core Enhancements
- Multiple analysis modes  
- Better error handling  
- Syntax highlighting  
- Line-by-line mode  
- Better UI/UX  

---

## ğŸŸ© Stage 2 â€” Distributed Architecture
- Redis caching  
- Queue workers (Celery / RQ)  
- Background tasks  
- Job-status API  
- Rate limiting  

---

## ğŸŸ§ Stage 3 â€” ETL + Spark Intelligence Layer
- Parse PySpark DAG  
- Detect transformations/actions  
- Detect shuffles  
- Anti-pattern detection  
- Auto documentation  
- Data lineage  

---

## ğŸŸ¥ Stage 4 â€” Production Deployment
- Prod Docker builds  
- Logging + structured logs  
- Prometheus metrics  
- OpenTelemetry tracing  
- CI/CD  
- Deployment (Fly.io / Render / Railway)  
- API gateway  

---

## ğŸ“ˆ Future Improvements
- UI presets  
- Autocomplete  
- Multi-file upload  
- Version comparison  
- Auto diagrams  

---

## ğŸ“œ License
MIT
